---
title: "Untitled"
format: html
bibliography: references.bib
---

```{r}
#| label: setup
library(tidyverse)
source("R/fcn_utility_FLUXNET.R")
```

## Load Ameriflux daily

I suspect a lot of the memory issues are from 1) sourcing files that produce a large number of objects (e.g. `TimeSeriesPlot_fluxnet_fullrecord.R` and 2) the number of columns being read in for the daily data.
Here I just read in *only* the columns we need for this example.

```{r}
metadata <- load_fluxnet_metadata()
manifest <-
  discover_AMF_files(data_dir = here::here("data/FLUXNET/AMF")) %>%
  filter(dataset == "FULLSET", time_integral == "DD") %>%
  # Deduplicate by keeping just one file per siteâ€”the one with the later end_year
  group_by(site) %>%
  filter(end_year == max(end_year)) %>% 
  ungroup()
# daily <- load_fluxnet_data(manifest) # Don't actually want to read in everything due to memory issues
```

```{r}
flux_vars <- "GPP_NT_VUT_MEAN"
by_site <- manifest %>%
  group_by(site)

# read_csv(manifest$path, col_select = c(TIMESTAMP, all_of(flux_vars))) #doesn't work unfortunately

# apparently not all files have a GPP_NT_VUT_MEAN column, so uses any_of()
daily <- map2(manifest$path, manifest$site, \(path, site) {
  read_csv(
    path,
    col_select = c(TIMESTAMP, any_of(flux_vars)),
    show_col_types = FALSE
  ) %>%
    mutate(site = unique(site), .before = 1)
}) %>%
  list_rbind() %>%
  mutate(
    across(any_of(flux_vars), \(x) na_if(x, -9999)),
    date = ymd(TIMESTAMP),
    .after = site
  ) %>%
  select(-TIMESTAMP) %>%
  left_join(metadata %>% select(site, LOCATION_LAT), by = join_by(site))
daily
```

## Benchmarking

This is the original `detect_phenology_integral()` function definition (with a minor addition of not hard-coding the date column name):

```{r}
#| code-fold: true
detect_phenology_integral <- function(daily_data, knots = 10, date_var = "date_object", flux_var = "GPP_NT_VUT_REF") {
 
  daily_data <- daily_data %>%
    filter(!is.na(.data[[flux_var]])) %>%
    mutate(
      year = lubridate::year(.data[[date_var]]),
      DOY = lubridate::yday(.data[[date_var]]),
      hemisphere = if_else(LOCATION_LAT < 0, "SH", "NH"),
      climate_zone = case_when(
        abs(LOCATION_LAT) < 23.5 ~ "Tropical",
        abs(LOCATION_LAT) >= 23.5 & LOCATION_LAT >= 0 ~ "Temperate_North",
        abs(LOCATION_LAT) >= 23.5 & LOCATION_LAT < 0 ~ "Temperate_South",
        .default =  "Other"
      ),
      DOY_aligned = case_when(
        climate_zone == "Temperate_South" ~ (DOY + 182) %% 365,
        .default = DOY
      )
    )
  
  phenology_results <- list()
  issues_log <- list()
  
  unique_sites <- unique(daily_data$site)
  
  for (site_id in unique_sites) {
    site_data <- daily_data %>% filter(site == site_id)
    unique_years <- sort(unique(site_data$year))
    site_climate <- site_data$climate_zone[1]
    
    for (yr in unique_years) {
      year_data <- site_data %>% filter(year == yr)
      prev_data <- site_data %>% filter(year == yr - 1) %>% tail(30)
      next_data <- site_data %>% filter(year == yr + 1) %>% head(30)
      padded_data <- bind_rows(prev_data, year_data, next_data)
      
      padded_data <- padded_data %>%
        arrange(.data[[date_var]]) %>%
        mutate(cumflux = cumsum(.data[[flux_var]]))
      
      if (nrow(padded_data) < knots) next
      
      tryCatch({
        spline_fit <- smooth.spline(x = 1:nrow(padded_data), y = padded_data$cumflux, df = knots)
        deriv_vals <- predict(spline_fit, deriv = 1)$y
        
        smoothed_gpp <- tibble(
          site = site_id,
          date = padded_data[[date_var]],
          DOY = padded_data$DOY,
          DOY_aligned = padded_data$DOY_aligned,
          year = padded_data$year,
          smoothed_flux = deriv_vals
        ) %>% filter(year == yr)
        
        max_flux <- max(smoothed_gpp$smoothed_flux, na.rm = TRUE)
        threshold_20 <- 0.2 * max_flux
        
        sos <- smoothed_gpp %>% filter(smoothed_flux >= threshold_20) %>% slice_head(n = 1)
        pos <- smoothed_gpp %>% filter(smoothed_flux == max_flux)
        eos <- smoothed_gpp %>% filter(DOY > pos$DOY[1] & smoothed_flux <= threshold_20) %>% slice_head(n = 1)
        
        SOS_val <- sos$DOY[1]
        POS_val <- pos$DOY[1]
        EOS_val <- eos$DOY[1]
        
        issue_reason <- NULL
        if (is.na(SOS_val) | is.na(POS_val) | is.na(EOS_val)) {
          issue_reason <- "NA in SOS/POS/EOS"
        } else if (SOS_val == 1) {
          issue_reason <- "SOS = 1 (possibly spurious)"
        } else if (site_climate != "Tropical" && (SOS_val >= POS_val || POS_val >= EOS_val)) {
          issue_reason <- "Order violation (SOS >= POS or POS >= EOS)"
        }
        
        if (!is.null(issue_reason)) {
          issues_log[[length(issues_log) + 1]] <- tibble(
            site = site_id,
            year = yr,
            SOS = SOS_val,
            POS = POS_val,
            EOS = EOS_val,
            reason = issue_reason
          )
        }
        
        phenology_results[[length(phenology_results) + 1]] <- tibble(
          site = site_id,
          year = yr,
          SOS = SOS_val,
          POS = POS_val,
          EOS = EOS_val
        )
      }, error = function(e) {})
    }
  }
  
  return(list(
    phenology_df = bind_rows(phenology_results),
    issues_df = bind_rows(issues_log)
  ))
}
```

For loops are sometimes not very memory efficient, and I don't see why the issue flags need to be in a separate data frame (please let me know if this is an issue).  I think I can improve on this.

```{r}
#' Apply to a df with a `site` column
detect_phenology_integral2 <- function(
  daily,
  knots = 10,
  date_var = "date",
  flux_var = "GPP_NT_VUT_MEAN",
  keep_data = FALSE
) {
  daily %>%
    group_split(site) %>%
    map(\(df) {
      detect_phenology_integral_site(
        df,
        knots = knots,
        date_var = date_var,
        flux_var = flux_var,
        keep_data = keep_data
      )
    }) %>%
    list_rbind()
}

#' Just for one site
detect_phenology_integral_site <- function(
  daily_site,
  knots = 10,
  date_var = "date",
  flux_var = "GPP_NT_VUT_MEAN",
  keep_data = FALSE
) {
  daily_site <- daily_site %>%
    filter(!is.na(.data[[flux_var]])) %>%
    mutate(
      year = lubridate::year(.data[[date_var]]),
      DOY = lubridate::yday(.data[[date_var]]),
      hemisphere = if_else(LOCATION_LAT < 0, "SH", "NH"),
      climate_zone = case_when(
        abs(LOCATION_LAT) < 23.5 ~ "Tropical",
        abs(LOCATION_LAT) >= 23.5 & LOCATION_LAT >= 0 ~ "Temperate_North",
        abs(LOCATION_LAT) >= 23.5 & LOCATION_LAT < 0 ~ "Temperate_South",
        .default = "Other"
      )
    )
  if (nrow(daily_site) == 0) {
    return(tibble())
  }
  years <-
    unique(daily_site$year) %>%
    sort()

  smooths <- map(years, \(focal_year) {
    start <- make_date(focal_year) - days(30)
    end <- ceiling_date(make_date(focal_year), "year") + days(29)

    # TODO need to actually split the data differently in the southern 
    # hemisphere so the "center" is 6-months offset
    padded_data <- daily_site %>%
      filter(between(date, start, end)) %>%
      arrange(date) %>%
      mutate(year = focal_year) %>%
      mutate(
        DOY_padded = case_when(
          year(date) == focal_year - 1 ~ (date - make_date(focal_year)) %>%
            as.numeric("days"),
          year(date) == focal_year + 1 ~ DOY + 365 + leap_year(focal_year),
          .default = DOY
        )
      ) %>%
      # TODO do we still need this?  I think this is just for plotting southern
      # hemisphere and northern hemisphere sites on the same x-axis, but I 
      # think I'd rather let the user do this outside of this function.
      mutate(
        DOY_aligned = case_when(
          climate_zone == "Temperate_South" ~ (DOY_padded + 182) %% 365,
          .default = DOY_padded
        )
      ) %>%
      mutate(cumflux = cumsum(GPP_NT_VUT_MEAN))

    m <- smooth.spline(
      x = 1:nrow(padded_data),
      y = padded_data$cumflux,
      df = 10
    )

    bind_cols(padded_data, smooth = predict(m, deriv = 1)$y)
  }) %>%
    set_names(years)

  out <- smooths %>%
    map(\(df) {
      max_flux <- max(df$smooth, na.rm = TRUE)
      df %>%
        summarize(
          SOS = DOY_padded[which.max(smooth >= max_flux * 0.2)],
          POS = DOY_padded[smooth == max_flux],
          EOS = DOY_padded[which.max(
            smooth <= max_flux * 0.2 & DOY_padded > POS
          )],
          climate_zone = unique(climate_zone),
          site = unique(site)
        ) %>%
        mutate(
          issues = case_when(
            is.na(SOS) | is.na(POS) | is.na(EOS) ~ "NA for SOS, POS, or EOS",
            SOS == 1 ~ "SOS == 1 (possibly spurrious)",
            climate_zone != "Tropical" &&
              (SOS >= POS |
                POS >= EOS) ~ "Order violation (SOS >= POS or POS >= EOS)"
          )
        )
    }) %>%
    list_rbind(names_to = "year") %>%
    mutate(year = as.integer(year)) %>%
    select(site, year, SOS, POS, EOS, issues)

  # optionaly keep the original data as a list column for plotting
  if (isTRUE(keep_data)) {
    smooths_min <- smooths %>%
      map(\(df) {
        df %>% select(date, all_of(flux_var), DOY, DOY_padded, DOY_aligned, smooth)
      })
    out$data <- smooths_min
  }

  out
}
```

```{r}
library(bench)
set.seed(100)
sample_sites <- sample(unique(daily$site), 10)
daily_sample <- daily %>% filter(site %in% sample_sites)
bm <- bench::mark(
  old = detect_phenology_integral(daily_sample, date_var = "date", flux_var = "GPP_NT_VUT_MEAN"),
  new = detect_phenology_integral2(daily_sample, date_var = "date", flux_var = "GPP_NT_VUT_MEAN"), 
  check = FALSE
)
bm
```

Yeah, that cuts the compute time reduces memory allocated a little bit.

How does it fare with the entire Ameriflux dataset?

```{r}
bench::bench_time(
  pheno_all <- detect_phenology_integral2(daily, date_var = "date", flux_var = "GPP_NT_VUT_MEAN", keep_data = TRUE)
)
pheno_all
```

Only about 30 seconds.

## Analysis

Let's take a look at the two sites suggested, US-Ha1 and US-SRM:

```{r}
pheno_all %>%
  mutate(flag = !is.na(issues)) %>%
  filter(site == "US-Ha1") %>%
  ggplot(aes(y = year)) +
  geom_pointrange(aes(x = POS, xmin = SOS, xmax = EOS, color = flag)) +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
  labs(title = "US-Ha1", color = "Issues", x = "Season range (DOY)")

pheno_all %>%
  mutate(flag = !is.na(issues)) %>%
  filter(site == "US-SRM") %>%
  ggplot(aes(y = year)) +
  geom_pointrange(aes(x = POS, xmin = SOS, xmax = EOS, color = flag)) +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
  labs(title = "US-SRM", color = "Issues", x = "Season range (DOY)")
 
```

US-Ha1 has a year with a flag and US-SRM has a year with a suspiciously early POS.  Let's take a closer look.

```{r}
pheno_all %>% 
  filter(site == "US-Ha1") %>%
  filter(!is.na(issues)) %>% 
  unnest(data) %>% 
  ggplot(aes(x = date)) +
  geom_point(aes(y = GPP_NT_VUT_MEAN), alpha = 0.3) +
  geom_line(aes(y = smooth), color = "blue") +
  labs(title = "US-Ha1 (1991)")
```

Here it looks like we just didn't capture the SOS because there isn't sufficient "padding" in the first year of data.

```{r}
pheno_all %>%
  filter(site == "US-SRM") %>%
  filter(year == 2020) %>%
  unnest(data) %>%
  ggplot(aes(x = date)) +
  geom_point(aes(y = GPP_NT_VUT_MEAN), alpha = 0.3) +
  geom_line(aes(y = smooth), color = "blue") +
  labs(title = "US-SRM (2020)")
```

Here we have some multi-modal seasonality, so it is unsurprising that this method didn't detect the season correctly. The first peak is at DOY 55 and is higher than the second peak.


```{r}
pheno_all %>%
  filter(site == "US-Ha1") %>%
  unnest(data) %>%
  ggplot(aes(x = DOY_padded, y = smooth, color = year, group = year)) +
  geom_line() +
  scale_color_continuous(n.breaks = 8) +
  labs(title = "US-Ha1", x = "DOY")

pheno_all %>%
  filter(site == "US-SRM") %>%
  unnest(data) %>%
  ggplot(aes(x = DOY_padded, y = smooth, color = year, group = year)) +
  geom_line() +
  scale_color_continuous(n.breaks = 8) +
  labs(title = "US-SRM", x = "DOY")
```

## What about other methods?

If there is interest, `detect_phenology_integral2()` could be modified to accept different thresholds (or even a vector of thresholds, outputing columns like `SOS_0.2`, `SOS_0.5`, etc.) or to use the first derivative method for detecting SOS and EOS.